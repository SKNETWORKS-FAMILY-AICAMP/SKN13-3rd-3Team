import pandas as pd
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnableParallel, RunnableLambda
from langchain.chat_models import ChatOpenAI
from langchain.output_parsers import StrOutputParser
from langchain.prompts import ChatPromptTemplate
from textwrap import dedent

load_dotenv()

### 1. 데이터 불러오기 및 문서 변환
df = pd.read_csv("oliveyoung.csv")

documents = [
    Document(page_content=row["txt"], metadata={
        "brand": row["brand"],
        "skin_type": row["skin_type"],
        "concerns": row["concerns"],
        "texture": row["texture"],
        "irritation": row["irritation"],
        "positive_emotions": row["positive_emotions"]
    }) for _, row in df.iterrows()
]

### 2. 임베딩 모델 설정 및 벡터 저장

# OpenAI 의 Embedding 모델 
from langchain_openai import OpenAIEmbeddings
e_model_id = "text-embedding-3-small"
# e_model_id = "text-embedding-3-large"
embedding_model = OpenAIEmbeddings(model=e_model_id)


# Ollama Embedding 모델 
from langchain_ollama import OllamaEmbeddings
e_model_id = "bge-m3"
embedding_model = OllamaEmbeddings(model=e_model_id)


# Huggingface Embedding 모델 
from langchain_huggingface import HuggingFaceEmbeddings
e_model_id = "intfloat/multilingual-e5-large"
embedding_model = HuggingFaceEmbeddings(model=e_model_id)


# 벡터스토어 - Chroma
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embedding_model,
    persist_directory="./chroma_rag_db"
)

retriever = vectorstore.as_retriever()

# 유사도 계산? 
import numpy as np 
def cosine_similarity(v1:np.ndarray|list, v2:np.ndarray|list)-> float:
    v1 = np.array(v1)
    v2 = np.array(v2)
    return (v1@v2)/(np.linalg.norm(v1)* np.linalg.norm(v2))


# embedded_query 와 embedded_docs 간의 유사도 계산 
for i, ev in enumerate(embedded_docs):
    print(f"{i+1}.{cosine_similarity(ev,embedded_query)}")



### 3. 유저 질문 분석 → 필드 추출 # 최소 하나는 사용
def extract_fields_from_question(question: str) -> list:
    field_keywords = {
        "제품명": ["제품명", "이름", "상품", "제품"],
        "카테고리": ["카테고리", "스킨", "토너", "에센스" ,"세럼", "앰플", "크림", "로션", "미스트", "오일"],
        "성분": ["성분", "히알루론산", "비타민", "AHA", "BHA", "나이아신아마이드"],
        "피부타입": ["건성", "지성", "복합성", "수부지", "수분부족지성", "극지성", "극건성", "속건조", "피부타입"],
        "피부고민": ["보습", "주름", "여드름", "미백", "흉터", "진정", "잡티", "흔적", "피부 고민"],
        "감정": ["좋았어요", "최악", "후회", "별점", "만족", "불만", "감정", "긍정", "부정", "중립"],
        "제형": ["제형", "찐득", "흡수", "쫀득", "미끌", "텍스처", "끈적", "마무리감", "마무리", "부드럽다"],
        "자극도": ["순해요", "트러블", "자극", "뾰루지", "뒤집어짐"]
    }
    result = []
    for field, keywords in field_keywords.items():
        if any(kw in question for kw in keywords):
            result.append(field)
    return result if result else ["제품명"]

### 4. 필드 기반 context 구성
def build_context(docs, fields):
    context_lines = []
    for doc in docs:
        lines = []
        for field in fields:
            if field.lower() in doc.metadata:
                lines.append(f"{field}: {doc.metadata[field.lower()]}")
        context_lines.append(" | ".join(lines))
    return "\n".join(context_lines)

### 5. 프롬프트 템플릿
prompt_template = ChatPromptTemplate(
    [
      ("system", dedent("""
      당신은 올리브영 스킨케어 화장품 정보를 전문적으로 안내하는 AI 어시스턴트입니다.
      사용자 질문에 따라 카테고리, 성분, 피부타입, 제형, 자극도, 감정 정보 등을 바탕으로 정확한 화장품 정보를 제공합니다.
      사용자의 피부 고민과 선호에 맞춰 화장품을 정확하게 추천하고, 관련 리뷰 정보를 요약해주는 것이 주요 역할입니다.

      # Instruction(지켜야 할 규칙):
      1. 반드시 제공된 문서(context)의 정보만을 기반으로 답변하세요. 
         문서에 없는 사실을 추측하거나 임의로 생성하지 마세요.
      2. 화장품의 이름과 카테고리는 반드시 '카테고리'와 '제품명' 필드를 참고하여 확인하세요. 
         카테고리는 스킨/토너, 에센스/세럼/앰플, 크림, 로션, 미스트/오일로 구성되어 있습니다.
      3. 가능한 한 명확하고 간결하게 답변하세요.
      4. 화장품 정보를 안내할 때는 다음 순서를 지키세요: 
         - 제품명
         - 주요 성분 (가능한 경우)
         - 평점 또는 긍정 리뷰 요약
      5. 문장 스타일은 전문성과 친근함을 겸비한 대화체로 작성하세요. 
         예: “이 제품은 복합성 피부에 정말 잘 맞는다고 해요!”
      6. “문서에 따르면”, “문맥에서 보면”과 같은 표현은 사용하지 마세요. 자연스럽게 설명만 하세요.
      * 7. 제품 추천 응답 마지막에는 꼭 다음 문장을 포함하세요:
         “이 제품에 대한 사용자들의 후기를 알려드릴까요?”
      8. 질문이 모호하거나 정보가 부족할 경우, 필요한 정보를 정중하게 요청하세요.

      # Context(문서 요약 정보):
      {context}

      # Format Instruction(출력 형식 지시):
      {format_instructions}
      """)),
      ("human", "{query}")
    ]
)

### 6. RAG LCEL 구성
model = ChatOpenAI(model="gpt-4.1", temperature=1)

parallel_inputs = RunnableParallel(
    docs=RunnableLambda(lambda q: retriever.get_relevant_documents(q)),
    fields=RunnableLambda(lambda q: extract_fields_from_question(q)),
    question=RunnableLambda(lambda q: q),
    format_instructions=RunnableLambda(lambda _: "자연스러운 한국어 문장으로 응답하세요.")
)

# LCEL 전체 체인 구성
rag_chain = (
    parallel_inputs
    | RunnableLambda(lambda d: {
        "context": build_context(d["docs"], d["fields"]),
        "question": d["question"],
        "format_instructions": d["format_instructions"]
    })
    | prompt_template
    | model
    | StrOutputParser()
)

### 7. 실행 예시
query = "복합성 피부에 잘 맞는 수분 크림 추천해줘"
response = rag_chain.invoke(query)
print(response)
