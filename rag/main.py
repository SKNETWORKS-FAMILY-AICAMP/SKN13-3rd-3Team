import streamlit as st
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.runnables import RunnablePassthrough, RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.output_parsers import StrOutputParser

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Streamlit ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "storage" not in st.session_state:
    st.session_state.storage = {}

# ì±—ë´‡ ê°ì²´ ìƒì„± í•¨ìˆ˜
@st.cache_resource
def get_chatbot():
    def get_session_history(session_id):
        if session_id not in st.session_state.storage:
            st.session_state.storage[session_id] = InMemoryChatMessageHistory()
        return st.session_state.storage[session_id]

    # ëª¨ë¸ ë° ë²¡í„°ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê¸°
    embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")
    vector_db = FAISS.load_local(
        "faiss_oliveyoung_reviews",
        embedding_model,
        allow_dangerous_deserialization=True
    )

    # LLM
    llm = ChatOpenAI(model='gpt-4.1', temperature=0.7)

    # í”„ë¡¬í”„íŠ¸ ì •ì˜
    system_prompt = """
    ë‹¹ì‹ ì€ ì˜¬ë¦¬ë¸Œì˜ ìŠ¤í‚¨ì¼€ì–´ í™”ì¥í’ˆ ì •ë³´ë¥¼ ì „ë¬¸ì ìœ¼ë¡œ ì•ˆë‚´í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ì‚¬ìš©ì ì§ˆë¬¸ì— ë”°ë¼ ì¹´í…Œê³ ë¦¬, ì„±ë¶„, í”¼ë¶€íƒ€ì…, ì œí˜•, ìê·¹ë„, ê°ì • ì •ë³´ ë“±ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ í™”ì¥í’ˆ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ í”¼ë¶€ ê³ ë¯¼ê³¼ ì„ í˜¸ì— ë§ì¶° ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” í™”ì¥í’ˆì„ 2~3 ê°€ì§€ ì¶”ì²œí•˜ê³ , ê´€ë ¨ ë¦¬ë·° ì •ë³´ë¥¼ ìš”ì•½í•´ì£¼ëŠ” ê²ƒì´ ì£¼ìš” ì—­í• ì…ë‹ˆë‹¤.

    # Instruction:
    1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ(context)ì˜ ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
    2. í™”ì¥í’ˆì˜ ì´ë¦„ê³¼ ì¹´í…Œê³ ë¦¬ëŠ” ë°˜ë“œì‹œ 'ì¹´í…Œê³ ë¦¬'ì™€ 'ì œí’ˆëª…' í•„ë“œë¥¼ ì°¸ê³ í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”.
    3. ê°€ëŠ¥í•œ í•œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
    4. í™”ì¥í’ˆ ì •ë³´ë¥¼ ì•ˆë‚´í•  ë•ŒëŠ” ë‹¤ìŒ ìˆœì„œë¥¼ ì§€í‚¤ì„¸ìš”:
       - ì œí’ˆëª…
       - ì£¼ìš” ì„±ë¶„ (ê°€ëŠ¥í•œ ê²½ìš°)
       - "í•´ë‹¹í•˜ëŠ” ê° ì œí’ˆëª…"ì˜ í‰ì  ë˜ëŠ” ê¸ì • ë¦¬ë·° ìš”ì•½
    5. ë¬¸ì¥ ìŠ¤íƒ€ì¼ì€ ì „ë¬¸ì„±ê³¼ ì¹œê·¼í•¨ì„ ê²¸ë¹„í•œ ëŒ€í™”ì²´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    6. â€œë¬¸ì„œì— ë”°ë¥´ë©´â€, â€œë¬¸ë§¥ì—ì„œ ë³´ë©´â€ê³¼ ê°™ì€ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
    7. ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ì •ë³´ê°€ ë¶€ì¡±í•  ê²½ìš°, í•„ìš”í•œ ì •ë³´ë¥¼ ì •ì¤‘í•˜ê²Œ ìš”ì²­í•˜ì„¸ìš”.
    8. ì¶œë ¥ì€ ë³´ê¸° ì‰½ê²Œ ì¤„ë°”ê¿ˆì„ í•´ì„œ ì „ë‹¬í•´ì£¼ì„¸ìš”.
    9. ì œí’ˆ ì¶”ì²œ ë° ì„¤ëª…ì€ ì„¸ ê°œ ì •ë„ ì¶œë ¥í•´ì£¼ì„¸ìš”.

    # Context:
    {context}

    # ì§ˆë¬¸:
    {question}
    """

    prompt_template = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}")
    ])

    # RAG ì²´ì¸ ì •ì˜
    def retrieve_documents(inputs):
        query = inputs["question"]

        # 1. ì œí’ˆëª… ì¶”ì¶œ
        product_extraction_prompt = f"""
        ì•„ë˜ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì¶”ì²œí•´ì•¼ í•  ì œí’ˆëª… 2~3ê°€ì§€ë¥¼ ëª…í™•í•˜ê²Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
        ë‹¨, ì œí’ˆëª…ë§Œ ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„ëœ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
        
        ì‚¬ìš©ì ì§ˆë¬¸: {query}
        """
        product_llm = ChatOpenAI(model='gpt-4.1', temperature=0)
        extracted = product_llm.invoke(product_extraction_prompt)
        product_names = list(set([p.strip() for p in extracted.content.split(",")]))[:3]

        docs = vector_db.similarity_search(query, k=15)
        
        context_docs = []
        for doc in docs:
            if any(product in doc.page_content for product in product_names):
                context_docs.append(doc.page_content[:1000])
                if len(context_docs) >= 5:
                    break  # ë„ˆë¬´ ë§ìœ¼ë©´ ì¤‘ë‹¨

        return "\n\n".join(context_docs)

    rag_chain = (
        RunnablePassthrough.assign(
            context=retrieve_documents,
            question=lambda x: x["question"]
        )
        | prompt_template
        | llm
        | StrOutputParser()
    )

    return RunnableWithMessageHistory(
        runnable=rag_chain,
        get_session_history=get_session_history,
        input_messages_key="question",
        history_messages_key="history"
    )

# ì±—ë´‡ ì‹¤í–‰
chain = get_chatbot()

# Streamlit UI ì„¤ì •
st.set_page_config(page_title="í™”ì¥í’ˆ ì¶”ì²œ AI", layout="wide")
st.title("ğŸ’„ ì˜¬ë¦¬ë¸Œì˜ ìŠ¤í‚¨ì¼€ì–´ ì¶”ì²œ ì±—ë´‡")

# ì´ì „ ëŒ€í™” ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ì‚¬ìš©ì ì…ë ¥
prompt = st.chat_input("ğŸ” ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ê±´ì„± í”¼ë¶€ì— ì¢‹ì€ ì—ì„¼ìŠ¤ ì¶”ì²œí•´ì¤˜)")

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    with st.chat_message("ai"):
        message_placeholder = st.empty()
        full_message = ""
        output_generator = chain.stream(
            {"question": prompt},
            config={"configurable": {"session_id": "user-session"}}
        )
        for chunk in output_generator:
            full_message += chunk
            message_placeholder.write(full_message)

        st.session_state.messages.append({"role": "ai", "content": full_message})